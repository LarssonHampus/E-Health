{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction \n\nThis project will follow the approach presented in the deeplearning.ai coursera specilization. The project will aim to follow an iterative approach by first applying a simple model. The purpose of this model is not necessarily to be the final model of this problem. Instead, it is used to creative a starting point to then iterativly alter the model and (hopefully) improve it. \n\n\n## The Goal \n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\n\n\n## The Data\n\"In this dataset, you are provided with a large number of small pathology images to classify. Files are named with an image id. The train_labels.csv file provides the ground truth for the images in the train folder. You are predicting the labels for the images in the test folder. A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. Tumor tissue in the outer region of the patch does not influence the label. This outer region is provided to enable fully-convolutional models that do not use zero-padding, to ensure consistent behavior when applied to a whole-slide image.\" \n\nThe data used in this project can be found here: https://www.kaggle.com/c/histopathologic-cancer-detection/notebooks \n\n\n\n## Acknowledgements\n This project was done after finishing the (excellt) specilization course by deeplearning.ai on coursera. Thus, the project was approached similarly to Andrew Ng's approach in this course. \n\n\nhttps://www.kaggle.com/vbookshelf/cnn-how-to-use-160-000-images-without-crashing\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries and Files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\nfrom glob import glob\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nimport cv2\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25802a26e4afba8f6dc42754f7f61da4c8cfea5c"},"cell_type":"markdown","source":"### What files are available?"},{"metadata":{"trusted":true,"_uuid":"699bb899bde433ba20fb0d086fa0f33a0f61a250"},"cell_type":"code","source":"os.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c24d1662f8bd5fc4d8c57c29449990a3520cd96b"},"cell_type":"markdown","source":"### Labels as per csv file\n\n0 = no tumor tissue<br>\n1 =   has tumor tissue. <br>\n"},{"metadata":{"_uuid":"5a285343c286be191aa827ff9b836b67821176a5"},"cell_type":"markdown","source":"### How many images are in each folder?"},{"metadata":{"trusted":true,"_uuid":"54461212efed65ac377369a468c80e7d708010f4"},"cell_type":"code","source":"\nprint(len(os.listdir('../input/train')))\nprint(len(os.listdir('../input/test')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b90854e07d495d9f945a0e40189fd32a0c32bff5"},"cell_type":"markdown","source":"### Create a Dataframe containing all images"},{"metadata":{"trusted":true,"_uuid":"e9c9f40ffab35044641b0dc7d9b18609af1aa25e"},"cell_type":"code","source":"df_data = pd.read_csv('../input/train_labels.csv')\nprint(df_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"_uuid":"cfbd53b7f8ea1929952ffed6221b380012618e32"},"cell_type":"markdown","source":"## Check the class distribution\n\n\nRemember that the label '0' denotes non tumor and label '1' denotes tumor. "},{"metadata":{"trusted":true,"_uuid":"e18560bf69d3dfc0c4772e7c79bb119fd2eb634b"},"cell_type":"code","source":"df_data['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nsns.countplot(x='label', data=df_data)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6efe2e5de99c4bf92079b1a7d0b892d30fc9d518"},"cell_type":"markdown","source":"## Display a random sample of train images  by class"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1c5143f227da4262eafce8cf0210a02c8072fb8e"},"cell_type":"code","source":"# source: https://www.kaggle.com/gpreda/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['id'] + '.tif'\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd38bcfb5839975e4fee9e70b93d42c29c1b5d2e"},"cell_type":"code","source":"IMAGE_PATH = '../input/train/' \n\nnum_pictures = 8\n\ndraw_category_images('label',num_pictures, df_data, IMAGE_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a non cancer reasearcher I have no idea what it is in the images that is the tumor cells. It looks like some cells that are larger are a factor that can determine if it is cancer or not. Perhaps also the color. Some images that have a lot of white area seems to be labeled as a tumor cell. "},{"metadata":{},"cell_type":"markdown","source":"# Data Split \n\nWe will downsample (see below) such that we have approximately 175k images. If we take create a validation set of 10 % we have about 17.5k images which should be enought, atleast for now, to give a decent spread of images to not overfit on the training data. also this gives us more training data to work with (~160k images).   \n\nhttps://cs230.stanford.edu/blog/split/"},{"metadata":{"_uuid":"1da4226777aefe65b1bb3430208ea91ea7ca7d9a"},"cell_type":"markdown","source":"## Create the Train and Validation/development Sets"},{"metadata":{"_uuid":"c1150500d2772b7f36cdaa5aa5fd7f0fb4a72628"},"cell_type":"markdown","source":"We will downsample the number of non tumor images to equal the number of tumor images. This is done for two reasons. More images require more computations which requires more computational power. A non balanced dataset can give the impression that a model is good when it is not. \n\nDownsides to this is ofcourse that we lose data."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nIMAGE_SIZE = 96 # the images are 96 x 96.\nIMAGE_CHANNELS = 3 # RGB \n\nSAMPLE_SIZE = df_data['label'].value_counts().min()  # the number of images we use from each of the two classes. In this case we downsample the number of non tumors. \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Downsample"},{"metadata":{"trusted":true,"_uuid":"270fc18640b552ecc3cb0e1dd3036441db7a4a2b"},"cell_type":"code","source":"# take a random sample of class 0 with size equal to num samples in class 1\ndf_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n# filter out class 1\ndf_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n\n# concat the dataframes\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n# shuffle\ndf_data = shuffle(df_data)\n\ndf_data['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train / validation split \n\nWe split such that the number of positve and negative samples are equal in the validation data. "},{"metadata":{"trusted":true,"_uuid":"15ba9792e6a370b7560330af15b3cfe21185c1cb"},"cell_type":"code","source":"# train_test_split\n\n# stratify=y creates a balanced validation set.\ny = df_data['label']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Percentage of images labeled as \"0\" in the training set:', round(df_train['label'].value_counts()[0] / df_train.shape[0],2))\nprint('Percentage of images labeled as \"1\" in the training set:', round(df_train['label'].value_counts()[1] / df_train.shape[0],2))\nprint('--'*30)\nprint('Percentage of images labeled as \"0\" in the validation set:',round(df_val['label'].value_counts()[0] / df_val.shape[0],2))\nprint('Percentage of images labeled as \"1\" in the validation set:',round(df_val['label'].value_counts()[1] / df_val.shape[0],2))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create dictionary "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Create directories\ntrain_path = 'base_dir/train'\nvalid_path = 'base_dir/valid'\ntest_path = '../input/test'\nfor fold in [train_path, valid_path]:\n    for subf in [\"0\", \"1\"]:\n        os.makedirs(os.path.join(fold, subf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the id as the index in df_data\ndf_data.set_index('id', inplace=True)\ndf_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image in df_train['id'].values:\n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname = image + '.tif' \n    label = str(df_data.loc[image,'label']) # get the label for a certain image\n    src = os.path.join('../input/train', fname)\n    dst = os.path.join(train_path, label, fname)\n    shutil.copyfile(src, dst)\n\nfor image in df_val['id'].values:\n    fname = image + '.tif'\n    label = str(df_data.loc[image,'label']) # get the label for a certain image\n    src = os.path.join('../input/train', fname)\n    dst = os.path.join(valid_path, label, fname)\n    shutil.copyfile(src, dst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\nIMAGE_SIZE = 96\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 32\nval_batch_size = 32\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\n\ndatagen = ImageDataGenerator(preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n                            horizontal_flip=True,\n                            vertical_flip=True)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='binary')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='binary')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='binary',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b789103dec6faf61148e291c59f9c7f13d1344a"},"cell_type":"markdown","source":"# Model\n\n\nThis is an iterative process. As it would be to demanding to write out all iterations, the text below will is descreptive of the process I followed. In this case the human error is estimated to be the bayes error which is estimated to be the best results on Kaggle. Preferably the iterative process would have been done, atleast in part, in code. However, due to the lack of computational resources I will manually try to improve the model. \n\n\n**Improving your model performance**\n* The two fundamental asssumptions of supervised learning:\n    - You can fit the training set pretty well. This is roughly saying that you can achieve low avoidable bias.\n    - The training set performance generalizes pretty well to the dev/test set. This is roughly saying that variance is not too bad.\n        * To improve your deep learning supervised system follow these guidelines:\n    - Look at the difference between human level error and the training error - avoidable bias.\n    - Look at the difference between the dev/test set and training set error - Variance.\n    - If avoidable bias is large you have these options:\n        1. Train bigger model.\n        2. Train longer/better optimization algorithm (like Momentum, RMSprop, Adam).\n        3. Find better NN architecture/hyperparameters search.\n    - If variance is large you have these options:\n        1. Get more training data.\n        2. Regularization (L2, Dropout, data augmentation).\n        3. Find better NN architecture/hyperparameters search.\n        \n        \n        \n        \n        \n        \n Furtermore, we are going to use transfer learning. Transfer learning gives us the benefit of using pre trained models that have been show to be successfull in similair tasks. This is especially beneficial due to time and computational restraints. "},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_models = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 1 - AlexNet\n\n16041/16041 [==============================] - 730s 45ms/step - loss: 0.1169 - acc: 0.9554 - val_loss: 0.2073 - val_acc: 0.9245\n10 epochs, batch_size  =10 \n\n\n\n\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AlexNet = keras.models.Sequential([\n    keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(96,96,3)),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(filters=384, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(filters=256, kernel_size=(1,1), strides=(1,1), activation='relu', padding=\"same\"),\n    keras.layers.BatchNormalization(),\n    keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(4096, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(4096, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AlexNet.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epoch = 15\nfilepath = \"AlexNet.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='min')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='min', min_lr=0.00001)\n\nearly_stop = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, mode='auto')\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr,early_stop]\n\nhistory = AlexNet.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=num_epoch,\n                    callbacks=callbacks_list,\n                     verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AlexNet.load_weights('AlexNet.h5')\nsaved_models['AlexNet'] = AlexNet ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 2 - ResNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\nfrom keras.layers import Conv2D, MaxPool2D\nfrom keras.applications.resnet50 import ResNet50\n\ndropout_fc = 0.5\n\nconv_base = ResNet50(weights = 'imagenet', include_top = False, input_shape = (96,96,3))\n\nresnet = Sequential()\nresnet.add(conv_base)\nresnet.add(Flatten())\nresnet.add(Dense(512, use_bias=False))\nresnet.add(BatchNormalization())\nresnet.add(Activation(\"relu\"))\nresnet.add(Dropout(dropout_fc))\nresnet.add(Dense(1, activation = \"sigmoid\"))\n\nresnet.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\nresnet.compile(optimizers.Adam(0.001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epoch = 50\nfilepath = \"ResNet50.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='min')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='min', min_lr=0.00001)\n\nearly_stop = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, mode='auto')\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr,early_stop]\n\nhistory = resnet.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=num_epoch,\n                    callbacks=callbacks_list,\n                     verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet.load_weights('ResNet50.h5')\nsaved_models['ResNet50'] = resnet ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 3 - DenseNet\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation,BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\ndropout_fc = 0.5\nconv_base = tf.keras.applications.DenseNet121(weights = 'imagenet', include_top = False, input_shape = (96,96,3))\n\ndensenet = keras.Sequential()\ndensenet.add(conv_base)\ndensenet.add(Flatten())\ndensenet.add(Dense(256, use_bias=False))\ndensenet.add(BatchNormalization())\ndensenet.add(Activation(\"relu\"))\ndensenet.add(Dropout(dropout_fc))\ndensenet.add(Dense(1, activation = \"sigmoid\"))\ndensenet.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"densenet.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnum_epochs = 50\nfilepath = \"DenseNet.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='min')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='min', min_lr=0.00001)\n\nearly_stop = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, mode='auto')\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr,early_stop]\n\nhistory = densenet.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=num_epoch,\n                    callbacks=callbacks_list,\n                     verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"densenet.load_weights('DenseNet.h5')\nsaved_models['DenseNet'] = densenet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 4 - InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation,BatchNormalization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\ndropout_fc = 0.5\nconv_base = tf.keras.applications.InceptionV3(weights = 'imagenet', include_top = False, input_shape = (96,96,3))\n\ninceptionnet = keras.Sequential()\ninceptionnet.add(conv_base)\ninceptionnet.add(Flatten())\ninceptionnet.add(Dense(256, use_bias=False))\ninceptionnet.add(BatchNormalization())\ninceptionnet.add(Activation(\"relu\"))\ninceptionnet.add(Dropout(dropout_fc))\ninceptionnet.add(Dense(1, activation = \"sigmoid\"))\ninceptionnet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inceptionnet.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnum_epochs = 50\nfilepath = \"IncepionNet.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='min')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='min', min_lr=0.00001)\n\nearly_stop = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, mode='auto')\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr,early_stop]\n\nhistory = inceptionnet.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=num_epoch,\n                    callbacks=callbacks_list,\n                     verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inceptionnet.load_weights('IncepionNet.h5')\nsaved_models['IncepionNet'] = inceptionnet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make a test prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.rmtree('base_dir') # free up space\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#[CREATE A TEST FOLDER DIRECTORY STRUCTURE]\n\n# We will be feeding test images from a folder into predict_generator().\n# Keras requires that the path should point to a folder containing images and not\n# to the images themselves. That is why we are creating a folder (test_images) \n# inside another folder (test_dir).\n\n# test_dir\n    # test_images\n\n# create test_dir\ntest_dir = 'test_dir'\nos.mkdir(test_dir)\n    \n# create test_images inside test_dir\ntest_images = os.path.join(test_dir, 'test_images')\nos.mkdir(test_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_list = os.listdir('../input/test')\n\nfor image in test_list:\n    \n    fname = image\n    \n    # source path to image\n    src = os.path.join('../input/test', fname)\n    # destination path to image\n    dst = os.path.join(test_images, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path ='test_dir'\n\n\n# Here we change the path to point to the test_images folder.\n\ntest_gen = datagen.flow_from_directory(test_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saved_models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_test_images = 57458\n\n# make sure we are using the best epoch\n#model.load_weights('model.h5')\n\npredictions = saved_models['ResNet'].predict_generator(test_gen, steps=num_test_images, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds = pd.DataFrame(predictions)\ndf_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This outputs the file names in the sequence in which \n# the generator processed the test images.\ntest_filenames = test_gen.filenames\n\n# add the filenames to the dataframe\ndf_preds['file_names'] = test_filenames\n\ndf_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_id(x):\n    \n    # split into a list\n    a = x.split('/')\n    # split into a list\n    b = a[1].split('.')\n    extracted_id = b[0]\n    \n    return extracted_id\n\ndf_preds['id'] = df_preds['file_names'].apply(extract_id)\n\ndf_preds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = df_preds[0]\n\n# get the id column\nimage_id = df_preds['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':image_id, \n                           'label':y_pred, \n                          }).set_index('id')\n\nsubmission.to_csv('resnet_pred.csv', columns=['label']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Ensemble "},{"metadata":{"trusted":true},"cell_type":"code","source":"InceptionNet = pd.read_csv('desktop/inceptionnet_pred.csv')\nDenseNet = pd.read_csv('desktop/DenseNet_preds.csv')\nResNet = pd.read_csv('desktop/resnet_pred.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preds = pd.DataFrame()\ndf_preds['id'] = DenseNet['id']\ndf_preds['label'] = ( InceptionNet['label']+ResNet['label'] + DenseNet['label']) / 3 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = df_preds['label']\nimage_id = df_preds['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':image_id, \n                           'label':y_pred, \n                          }).set_index('id')\n\nsubmission.to_csv('Ensemble_pred.csv', columns=['label']) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}